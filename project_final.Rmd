---
title: "Projec_final"
author: "Sajud, Mesut, Alvin"
date: "5/6/2020"
output: html_document
---

```{r}
require(tidyverse)
require(ggplot2)
require(broom)
require(geosphere)
require(modelr)
require(Hmisc)
require(mgcv)
require(caret)
```


#Loading Data
```{r}
train_data = read.csv("A:/Study/College/Graduate/2nd Year/4th Sem/Data Science II - Data Mining Algorithms And Applications/Project/cleantrain.csv") #The Actual Data was over 500,000 data, we reduced the data to 100,000 for efficiently loading and using data.

test_data = read.csv("A:/Study/College/Graduate/2nd Year/4th Sem/Data Science II - Data Mining Algorithms And Applications/Project/test.csv")
```

```{r}

anyNA(train_data)
anyNA(test_data)

summary(train_data)#we find that the fare_amount has negative values #also we have a few outlier longtitude and latitude

train_data = train_data%>%filter(fare_amount > 0) #filtering only to keep the positive fare_amount, and a zero fare_amount makes no sense

train_data  = train_data%>%filter(abs(pickup_latitude) <90 & abs(pickup_longitude) < 90 & dropoff_latitude <90) #the data had extreme values of longitutde and latitude

summary(train_data)
```


#Exploring the Data

We have to predict the fare amount from the train_data to the test data
```{r}
hist(train_data$fare_amount) #we see that the fare_amount is positively skewed

mean(train_data$fare_amount)

sd(train_data$fare_amount) #the Standard Deviation is around 9.7
```


#Now we have to decide what model has to be used and do we need to normalize the numerical value or add a few variables.
#like converting into logarthmic value and reconverting back.
#As we can see that we have longitude and latitudes for pick_up and drop_off we can get the distance between them
```{r}
options(scipen = 999)
distance=distHaversine(train_data[,c(4,5)],train_data[,c(6,7)]) #using distHaversine function from the geosphere library we were able to find the distance in metric using the longitutde and latitude respectively

train_data1 = train_data%>%mutate(distance)
```


#We have the pickup time taking it as the start hour we can use if the pickup time affects the fare_amount
```{r}
train_data2=train_data1%>%mutate(start_hour=format(as.POSIXct(train_data1$pickup_datetime, format="%Y-%m-%d %H:%M:%S"),format="%H.%M"),start_hour=as.numeric(start_hour))
```


#Now we have the start time for each travel now we decided to categorize the timing to traffic density
#Assuming certain time_frames pertain High, Medium or Low Traffic
```{r}
train_data3 = train_data2%>%mutate(density=case_when(start_hour<7.00~'Low',(start_hour>=7.00&start_hour<=11.00)~'High',(start_hour>11.00&start_hour<16.00)~'Medium',(start_hour>=16.00&start_hour<=20.00)~'High',(start_hour>20.00)~'Medium'))
```


#Creating Dummy Variables
#Creating dummy variables since there are categorical data we convert it into random values of 0's and 1's to check if we can get a better prediction
```{r}
train_data3%>%filter(fare_amount == 0)

train_data4 = train_data3[-c(10003,27892),]

f = formula('~density-1')

nw =model.matrix(f, data = train_data3)

train_data5 = cbind(train_data4,nw)
```


#Adding the same columns for test_data

```{r}
test_data
distance=distHaversine(test_data[,c(3,4)],test_data[,c(5,6)]) #using distHaversine function from the geosphere library we were able to find the distance in metric using the longitutde and latitude respectively

test_data1 = test_data%>%mutate(distance)

test_data2=test_data1%>%mutate(start_hour=format(as.POSIXct(test_data1$pickup_datetime, format="%Y-%m-%d %H:%M:%S"),format="%H.%M"),start_hour=as.numeric(start_hour))

test_data3 = test_data2%>%mutate(density=case_when(start_hour<7.00~'Low',(start_hour>=7.00&start_hour<=11.00)~'High',(start_hour>11.00&start_hour<16.00)~'Medium',(start_hour>=16.00&start_hour<=20.00)~'High',(start_hour>20.00)~'Medium'))

nw2 = model.matrix(f, data = test_data3)

test_data4 = cbind(test_data3,nw2)
```



```{r}
#describe(train_data4) #checking the descriptive stats of the data
anyNA(train_data5) #checking if there are any NA values
train_data6 = train_data5%>%drop_na() #dropping and adding the data into a new data frame
```

#Using the leaps model to select the best parameters/predictors
```{r}
#install.packages('leaps')
require(leaps)
lps=regsubsets(fare_amount~distance+start_hour, data=train_data6)
plot(lps, scale='adjr2')
```


#Predicting using Linear regression model
```{r}

formula1 = fare_amount ~ distance + start_hour
linear_model = lm(formula1, data = train_data6)
summary(linear_model)

linear_prediction = predict(linear_model, test_data4)

linear_pred = test_data4%>%mutate(fare_amount = linear_prediction)%>%select(key,fare_amount)
```


#Predicting using GAM model
```{r}
gam_model1 = gam(fare_amount~s(distance + start_hour), data = train_data6)

prediction1 =predict(gam_model1,test_data4)


gam_pred = test_data4%>%mutate(fare_amount = prediction1)%>%select(key, fare_amount)

models = list(linear_model,gam_model1)

rsquare = map_dbl(models, ~rsquare(.x, test_data4))
```
We see that the GAM having the issue of over-fitting we get a better R-square in comparison to linear model
```{r}
rmse_linear = modelr::rmse(linear_model, test_data4) #checking the RMSE for linear model

rmse_gam = modelr::rmse(gam_model1, test_data4) #checking the RMSE for GAM
```
#POST
We found that out of all the variables to get a better fit we had to use the variables distance(calculated) and start_hour, which gives out a better prediction, and also Normalizing the data was not needed.
In Future aspects we could consider that if we could find the time traveled throught that distance would help.
In regards with SVM we tried converting the predictor to integer, but the data was not able to get through stating the error 'Vector over 5.5gb'

```{r}
#write_csv(linear_pred,"C:/Users/alvin/Desktop/linear_pred.csv")
#write_csv(gam_pred,"C:/Users/alvin/Desktop/gam_pred.csv")

```

